## ***Data Augmentation***
**Survey**
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Data Management for Machine Learning: A Survey](https://ieeexplore.ieee.org/document/9705125)|IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING(2023)|The document discusses the challenges faced in machine learning (ML) and how database techniques can address these challenges. The main points and key arguments of the document are as follows:1.ML requires high-quality training data, which is expensive to acquire. Database techniques can help in data preparation by improving data discovery, data cleaning, and data labeling.2.Training ML models with large amounts of data and complex structures can be inefficient. Database techniques can accelerate the training process by using materialization or parallelism, and also aid in feature selection and model selection.3.Managing trained models is a challenge as they contain a lot of information. Database techniques can be used for model storage, query, deployment, and debugging.4.The document provides a comprehensive review of existing studies in three aspects: data preparation (Pre-ML), model training and inference (In-ML), and model management (Post-ML).5.In the Pre-ML phase, the document discusses data discovery, data cleaning, and data labeling techniques.6.In the In-ML phase, the document focuses on feature selection, model selection, and acceleration of model training and inference.7.The document also highlights the importance of database techniques in optimizing linear algebra computations and transforming LA operations to relation algebra.|
|[A survey on data augmentation for text classification](https://arxiv.org/abs/2107.03158)|ACM Computing Surveys (2022)|This document is a survey on data augmentation for text classification. The authors provide a comprehensive overview of data augmentation methods for textual classification and categorize more than 100 methods into 12 different groups. They also discuss the goals and applications of data augmentation, the taxonomy and categorization of methods, and provide a state-of-the-art review. The authors highlight the importance of data augmentation in addressing challenges such as limited training data, regularization, and privacy protection. They also discuss the relationship between data augmentation and deep learning, as well as the limitations and future research directions in this field.|
|[An Empirical Survey of Data Augmentation for Limited Data Learning in NLP](https://arxiv.org/abs/2106.07499)|Transactions of the Association for Computational Linguistics 11 (2023)|The document is an empirical survey of data augmentation methods for limited data learning in natural language processing (NLP). The authors highlight the challenge of applying NLP models in low-resource settings or novel tasks where labeled data is scarce. Data augmentation methods have been explored as a solution to improve data efficiency in NLP. However, there has been no systematic overview of data augmentation for NLP in the limited labeled data setting. The authors provide a comprehensive survey of recent progress on data augmentation for NLP, categorizing methods into token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations. They conduct experiments on 11 datasets covering various NLP tasks and draw conclusions to help practitioners choose appropriate augmentations in different settings. The authors also discuss the current challenges and future directions for limited data learning in NLP.|
|[Data augmentation for deep graph learning: A survey](https://dl.acm.org/doi/abs/10.1145/3575637.3575646)|ACM SIGKDD Explorations Newsletter 24.2 (2022)|The document is a survey on data augmentation techniques for deep graph learning. The authors highlight the challenges of data noise and data scarcity in deep graph learning and discuss the importance of data augmentation to address these issues. They propose a taxonomy for graph data augmentation techniques and categorize them based on the augmented information modalities. The authors also summarize the applications of graph data augmentation in reliable graph learning and low-resource graph learning. They provide a hierarchical problem taxonomy and review the existing literature related to graph data augmentation for each problem. The document concludes by pointing out promising research directions and challenges in future research.|
|[A survey of data augmentation approaches for NLP](https://arxiv.org/abs/2105.03075)|arXiv preprint arXiv:2105.03075 (2021)|The document is a survey of data augmentation approaches for natural language processing (NLP). It highlights the increased interest in data augmentation in NLP due to low-resource domains, new tasks, and the popularity of large-scale neural networks. The survey aims to provide a comprehensive overview of data augmentation techniques for NLP by summarizing the literature in a structured manner. It discusses rule-based, example interpolation-based, and model-based approaches. It also explores the use of data augmentation in various NLP applications and tasks. The document concludes by outlining current challenges and directions for future research in data augmentation for NLP.|

**Relational data**
| Paper | Conference | Type | Remark |
| :---:| :---:| :---:| :---:|
|[Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approac](https://arxiv.org/abs/2010.13273)|ICDE 21|joinable table search|The document discusses the problem of finding joinable tables in data lakes and proposes a framework called PEXESO for efficient joinable table discovery. Traditional approaches for finding joinable tables are limited in dealing with misspellings, different formats, and capturing semantic joins. PEXESO addresses these limitations by using high-dimensional vectors to represent textual values and joining columns based on similarity predicates. The framework utilizes a block-and-verify method with pivot-based filtering to efficiently find joinable tables with similarity. It also incorporates a partitioning technique for large data lakes that cannot fit in main memory. Experimental evaluations on real datasets demonstrate that PEXESO identifies more tables than equi-joins and outperforms other similarity-based options, while also improving the performance of machine learning tasks. The proposed method is efficient and achieves significant speedup compared to exact baselines.|
|[Leva: Boosting Machine Learning Performance with Relational Embedding Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3514221.3517891))|SIGMOD 22|joinable table search|The document titled "Leva: Boosting Machine Learning Performance with Relational Embedding Data Augmentation" presents a system called Leva that enhances the performance of machine learning tasks over relational data. Leva constructs a relational embedding by representing relational data as a graph and using embedding methods to represent the graph as vectors. The embedding includes information from the entire database, which improves the performance of downstream machine learning tasks. The document highlights the challenges of feature engineering, data discovery, and integration in building training datasets and discusses the tradeoff between convenience and performance. Leva addresses these challenges by automatically constructing a relational embedding without prior knowledge of table keys or join paths. The document presents the main contributions of Leva, including relational embeddings for boosting ML models, an end-to-end system for generating relational embeddings, and an analysis of why the embedding works. The document concludes with an outline of the paper and references to related work in the field.|
|[DeepJoin: Joinable Table Discovery with Pre-trained Language Models](https://dl.acm.org/doi/10.14778/3603581.3603587)|PVLDB 23|joinable table search|The document proposes a deep learning model called DeepJoin for accurate and efficient joinable table discovery in data lake management. The model utilizes a pre-trained language model (PLM) to embed columns into vectors and employs an embedding-based retrieval approach to find joinable tables. The PLM is fine-tuned to embed columns in such a way that joinable columns are close to each other in the vector space. The search procedure is independent of the column size, and with the use of an approximate nearest neighbor search algorithm, the search time is sublinear in the repository size. The model is trained using techniques for preparing training data and data augmentation. Experimental results show that DeepJoin outperforms other approximate solutions in terms of precision and is even more accurate than an exact solution for semantic joins. Additionally, DeepJoin is significantly faster when equipped with a GPU. The document also provides a problem definition and preliminary information on equi-joins and semantic joins.|
|[Table Union Search on Open Data.](https://dl.acm.org/doi/abs/10.14778/3192965.3192973)|PVLDB 18|table union search|The document presents a solution to the table union search problem, which involves finding tables that can be combined with a query table within large repositories of open data. The authors propose a probabilistic approach that utilizes three statistical models to determine the unionability of attributes: set domains, semantic domains with values from an ontology, and natural language domains. They also introduce a data-driven method for selecting the best model for each attribute pair. The authors evaluate the accuracy of their solution using a benchmark of open data tables and demonstrate that their approach outperforms existing algorithms in terms of speed and accuracy, even when dealing with repositories containing over one million attributes.|
|[Dataset Discovery in Data Lakes](https://ieeexplore.ieee.org/document/9101607)|ICDE 20|table union search|The document discusses the problem of dataset discovery in data lakes and proposes an effective and efficient solution to address it. The authors highlight the increasing availability of datasets in data lakes without explicit knowledge of their conceptual relationships. They define a data lake as a repository of datasets with minimal metadata, such as attribute names and types. The paper focuses on the initial stage of dataset discovery, which involves identifying datasets that are most useful for data wrangling tasks. The authors propose a solution called D3L (Dataset Discovery in Data Lakes) that uses features of the values in a dataset to construct hash-based indexes. These indexes map the features into a uniform distance space, allowing for the measurement of relatedness between features and a target table. The approach considers various types of evidence, including attribute name similarity, attribute extent overlap, word-embedding similarity, format representation similarity, and domain distribution similarity. The authors provide a detailed description of the approach and report empirical results showing significant improvements in precision, recall, target coverage, indexing, and discovery times compared to prior work.|
|[Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning](https://dl.acm.org/doi/10.14778/3587136.3587146)|PVLDB 23|table union search|proposes Starmie, an end-to-end framework for dataset discovery from data lakes, with a focus on table union search. The framework utilizes a contrastive learning method to train column encoders from pre-trained language models, capturing rich contextual semantic information within tables. The cosine similarity between column embedding vectors is used as the column unionability score, and a filter-and-verification framework is proposed to compute the unionability score between two tables. Starmie outperforms existing solutions in the effectiveness of table union search and employs the HNSW index to accelerate query processing. The document highlights the challenges in dataset discovery and the contributions of Starmie in addressing these challenges.|

**Others**
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Chataug: Leveraging chatgpt for text data augmentation](https://arxiv.org/pdf/2302.13007v1.pdf)|arXiv preprint arXiv:2302.13007 (2023)|The document proposes a text data augmentation approach called ChatAug, which is based on the ChatGPT language model. The challenge of limited sample sizes in natural language processing (NLP) tasks, especially in few-shot learning scenarios, can be overcome by augmenting the training data. However, existing methods lack either correct labeling or diversity in the generated data. ChatAug rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples using ChatGPT. The augmented samples can then be used for downstream model training. Experimental results show that ChatAug outperforms state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of augmented samples. The document also discusses related work on data augmentation and few-shot learning. |
|[Data augmentation for spoken language understanding via pretrained language models](https://arxiv.org/abs/2004.13952)|arXiv preprint arXiv:2004.13952 (2020)|The document discusses a data augmentation method using pretrained language models to improve the training of spoken language understanding (SLU) models. The authors propose a semantically controlled generation approach, where a pretrained language model is used to generate synthetic training data based on dialogue acts. The method aims to increase the variability and accuracy of SLU models. Additionally, the authors address two overlooked scenarios of data scarcity in SLU: "Rich-in-Ontology" and "Rich-in-Utterance." In the Rich-in-Ontology scenario, the full ontology for the dialogue domain is given, but natural language utterances are lacking. In the Rich-in-Utterance scenario, there are abundant unlabelled utterances without annotated intents, slots, and values. The authors propose corresponding data augmentation solutions for both scenarios. Experimental results show that the proposed method outperforms baseline systems in terms of slot tagging and intent classification accuracies on the ATIS and Snips datasets.|
|[Effective Entity Augmentation By Querying External Data Sources](https://dl.acm.org/doi/abs/10.14778/3611479.3611535)|PVLDB 23|The document discusses the challenge of augmenting and enriching entities in datasets with relevant information from external data sources. It highlights the difficulties in formulating keyword queries to extract specific information from external sources and the limitations of current methods for information enrichment. The document proposes a progressive approach that leverages user feedback to learn how to retrieve information relevant to each entity in a dataset from external data sources. The approach aims to deliver relevant information quickly and with minimal expert intervention. The document also discusses the challenges of online learning of query policies and presents various methods and techniques to address these challenges. The proposed methods are evaluated through empirical studies using real-world datasets from different domains.|

