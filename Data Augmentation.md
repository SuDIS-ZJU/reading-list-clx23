## ***Data Augmentation***

**Relational data**
| Paper | Conference | Type | Remark |
| :---:| :---:| :---:| :---:|
|[Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional Similarity-Based Approac](https://arxiv.org/abs/2010.13273)|ICDE 21|joinable table search|The document discusses the problem of finding joinable tables in data lakes and proposes a framework called PEXESO for efficient joinable table discovery. Traditional approaches for finding joinable tables are limited in dealing with misspellings, different formats, and capturing semantic joins. PEXESO addresses these limitations by using high-dimensional vectors to represent textual values and joining columns based on similarity predicates. The framework utilizes a block-and-verify method with pivot-based filtering to efficiently find joinable tables with similarity. It also incorporates a partitioning technique for large data lakes that cannot fit in main memory. Experimental evaluations on real datasets demonstrate that PEXESO identifies more tables than equi-joins and outperforms other similarity-based options, while also improving the performance of machine learning tasks. The proposed method is efficient and achieves significant speedup compared to exact baselines.|
|[Leva: Boosting Machine Learning Performance with Relational Embedding Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3514221.3517891))|SIGMOD 22|joinable table search|The document presents a system called Leva that enhances the performance of machine learning tasks over relational data. Leva constructs a relational embedding by representing relational data as a graph and using embedding methods to represent the graph as vectors. The embedding includes information from the entire database, which improves the performance of downstream machine learning tasks. The document highlights the challenges of feature engineering, data discovery, and integration in building training datasets and discusses the tradeoff between convenience and performance. Leva addresses these challenges by automatically constructing a relational embedding without prior knowledge of table keys or join paths. The document presents the main contributions of Leva, including relational embeddings for boosting ML models, an end-to-end system for generating relational embeddings, and an analysis of why the embedding works. The document concludes with an outline of the paper and references to related work in the field.|
|[DeepJoin: Joinable Table Discovery with Pre-trained Language Models](https://dl.acm.org/doi/10.14778/3603581.3603587)|PVLDB 23|joinable table search|The document proposes a deep learning model called DeepJoin for accurate and efficient joinable table discovery in data lake management. The model utilizes a pre-trained language model (PLM) to embed columns into vectors and employs an embedding-based retrieval approach to find joinable tables. The PLM is fine-tuned to embed columns in such a way that joinable columns are close to each other in the vector space. The search procedure is independent of the column size, and with the use of an approximate nearest neighbor search algorithm, the search time is sublinear in the repository size. The model is trained using techniques for preparing training data and data augmentation. Experimental results show that DeepJoin outperforms other approximate solutions in terms of precision and is even more accurate than an exact solution for semantic joins. Additionally, DeepJoin is significantly faster when equipped with a GPU. The document also provides a problem definition and preliminary information on equi-joins and semantic joins.|
|[Table Union Search on Open Data.](https://dl.acm.org/doi/abs/10.14778/3192965.3192973)|PVLDB 18|table union search|The document presents a solution to the table union search problem, which involves finding tables that can be combined with a query table within large repositories of open data. The authors propose a probabilistic approach that utilizes three statistical models to determine the unionability of attributes: set domains, semantic domains with values from an ontology, and natural language domains. They also introduce a data-driven method for selecting the best model for each attribute pair. The authors evaluate the accuracy of their solution using a benchmark of open data tables and demonstrate that their approach outperforms existing algorithms in terms of speed and accuracy, even when dealing with repositories containing over one million attributes.|
|[Dataset Discovery in Data Lakes](https://ieeexplore.ieee.org/document/9101607)|ICDE 20|table union search|The document discusses the problem of dataset discovery in data lakes and proposes an effective and efficient solution to address it. The authors highlight the increasing availability of datasets in data lakes without explicit knowledge of their conceptual relationships. They define a data lake as a repository of datasets with minimal metadata, such as attribute names and types. The paper focuses on the initial stage of dataset discovery, which involves identifying datasets that are most useful for data wrangling tasks. The authors propose a solution called D3L (Dataset Discovery in Data Lakes) that uses features of the values in a dataset to construct hash-based indexes. These indexes map the features into a uniform distance space, allowing for the measurement of relatedness between features and a target table. The approach considers various types of evidence, including attribute name similarity, attribute extent overlap, word-embedding similarity, format representation similarity, and domain distribution similarity. The authors provide a detailed description of the approach and report empirical results showing significant improvements in precision, recall, target coverage, indexing, and discovery times compared to prior work.|
|[Semantics-aware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning](https://dl.acm.org/doi/10.14778/3587136.3587146)|PVLDB 23|table union search|The document proposes Starmie, an end-to-end framework for dataset discovery from data lakes, with a focus on table union search. The framework utilizes a contrastive learning method to train column encoders from pre-trained language models, capturing rich contextual semantic information within tables. The cosine similarity between column embedding vectors is used as the column unionability score, and a filter-and-verification framework is proposed to compute the unionability score between two tables. Starmie outperforms existing solutions in the effectiveness of table union search and employs the HNSW index to accelerate query processing. The document highlights the challenges in dataset discovery and the contributions of Starmie in addressing these challenges.|
|[SANTOS: Relationship-based Semantic Table Union Search](https://dl.acm.org/doi/10.1145/3588689)|SIGMOD 23|table union search|The document introduces a new approach to table search called SANTOS, which improves the accuracy of union search by considering semantic relationships between columns in a table. The document presents two methods to discover these semantic relationships: one using an existing knowledge base (KB) and the other using knowledge from the data lake itself. The authors show that SANTOS outperforms existing union search algorithms that rely on column-based semantics. They also demonstrate that a synthesized KB can improve the accuracy of union search by representing relationship semantics not found in an available KB. The document provides a new definition of unionability based on both attribute semantics and relationship semantics. The authors present their contributions, including the relationship-based semantic table union search, the KB solution, the synthesized KB solution, empirical evaluation, and benchmarks. The document also discusses related work on table union search and attribute annotation.|
|[Organizing Data Lakes for Navigation](https://dl.acm.org/doi/abs/10.1145/3318464.3380605)|SIGMOD 20|navigation||
|[TURL: Table Understanding through Representation Learning](https://dl.acm.org/doi/abs/10.1145/3542700.3542709)|SIGMOD Record 22|table understanding||
|[Retrieval-Based Transformer for Table Augmentation](https://arxiv.org/abs/2306.11843)|ACL 2023||The document introduces a novel approach to automatic data wrangling, specifically table augmentation tasks, such as row/column population and data imputation. The proposed approach is a retrieval augmented self-trained transformer model that reconstructs the original values or headers given partial tables as input. The model consists of a dense neural retrieval model and an end-to-end model for table augmentation tasks. The model is tested on the EntiTables benchmark and a new benchmark called WebTables, outperforming both supervised statistical methods and current transformer-based models. The key contributions of the paper are the introduction of the first end-to-end retrieval-based model for table augmentation, establishing a new state-of-the-art, and the creation of a new dataset for evaluation. The related work in the field is also discussed, highlighting different approaches to table augmentation tasks. The approach is based on retrieval augmented transformer models and builds on the concept of using non-parametric knowledge from an indexed corpus. The model consists of a table index, a retrieval component, and a reader or selection component. The table index is built from the training set, and the retrieval component is trained without ground truth on correct provenance. The reader component is an extractive approach, ensuring grounded predictions. The document provides a formal description of the input and ablation process for the table augmentation tasks.|
|[FakeTables: Using GANs to Generate Functional Dependency Preserving Tables with Bounded Real Data](https://www.ijcai.org/proceedings/2019/287)|IJCAI 19|generate new table|The document discusses the problem of data augmentation for tabular data. The authors propose a novel generative adversarial network (GAN) framework called ITS-GAN to generate synthetic tables that have similar statistics as the original table and preserve the functional dependencies of a released sub-table. The main contributions of the paper are:1. Introducing the problem of incomplete table synthesis (ITS) for tabular data augmentation.2. Proposing the ITS-GAN framework that incorporates two constraints: maintaining table statistics and preserving functional dependencies.3. Training autoencoders to model the record-level functional dependencies and incorporating them into the generator loss function.4. Customizing the discriminator's input calibration using the errors calculated by the pre-trained autoencoders.5. Theoretical analysis and empirical evaluations on two datasets (US Census Bureau data and US Bureau of Transportation Statistics data) showing that ITS-GAN outperforms state-of-the-art data augmentation approaches.|
|[Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples](https://arxiv.org/abs/2307.14565)|VLDB 23|pre-aug/generate new table|The document discusses the problem of non-relational tables and the difficulties they pose for querying using SQL-based tools. The authors conducted a survey of real spreadsheet-tables and web-tables and found that over 30% of these tables do not conform to the relational standard. To address this issue, the authors propose the Auto-Tables system, which automatically synthesizes pipelines with multi-step transformations to transform non-relational tables into standard relational forms. The system can successfully synthesize transformations for over 70% of test cases at interactive speeds, making it an effective tool for preparing data for analytics. The document provides examples of non-relational tables and the corresponding transformations required to make them relational. The authors also highlight the challenges faced by users in manually programming these transformations and the prevalence of related questions on forums. The Auto-Tables system aims to alleviate these challenges by automating the transformation process without requiring users to provide examples.|

**Survey**
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Data Management for Machine Learning: A Survey](https://ieeexplore.ieee.org/document/9705125)|IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING(2023)|The document discusses the challenges faced in machine learning (ML) and how database techniques can address these challenges. The main points and key arguments of the document are as follows:1.ML requires high-quality training data, which is expensive to acquire. Database techniques can help in data preparation by improving data discovery, data cleaning, and data labeling.2.Training ML models with large amounts of data and complex structures can be inefficient. Database techniques can accelerate the training process by using materialization or parallelism, and also aid in feature selection and model selection.3.Managing trained models is a challenge as they contain a lot of information. Database techniques can be used for model storage, query, deployment, and debugging.4.The document provides a comprehensive review of existing studies in three aspects: data preparation (Pre-ML), model training and inference (In-ML), and model management (Post-ML).5.In the Pre-ML phase, the document discusses data discovery, data cleaning, and data labeling techniques.6.In the In-ML phase, the document focuses on feature selection, model selection, and acceleration of model training and inference.7.The document also highlights the importance of database techniques in optimizing linear algebra computations and transforming LA operations to relation algebra.|
|[A survey on data augmentation for text classification](https://arxiv.org/abs/2107.03158)|ACM Computing Surveys (2022)|This document is a survey on data augmentation for text classification. The authors provide a comprehensive overview of data augmentation methods for textual classification and categorize more than 100 methods into 12 different groups. They also discuss the goals and applications of data augmentation, the taxonomy and categorization of methods, and provide a state-of-the-art review. The authors highlight the importance of data augmentation in addressing challenges such as limited training data, regularization, and privacy protection. They also discuss the relationship between data augmentation and deep learning, as well as the limitations and future research directions in this field.|
|[An Empirical Survey of Data Augmentation for Limited Data Learning in NLP](https://arxiv.org/abs/2106.07499)|Transactions of the Association for Computational Linguistics 11 (2023)|The document is an empirical survey of data augmentation methods for limited data learning in natural language processing (NLP). The authors highlight the challenge of applying NLP models in low-resource settings or novel tasks where labeled data is scarce. Data augmentation methods have been explored as a solution to improve data efficiency in NLP. However, there has been no systematic overview of data augmentation for NLP in the limited labeled data setting. The authors provide a comprehensive survey of recent progress on data augmentation for NLP, categorizing methods into token-level augmentations, sentence-level augmentations, adversarial augmentations, and hidden-space augmentations. They conduct experiments on 11 datasets covering various NLP tasks and draw conclusions to help practitioners choose appropriate augmentations in different settings. The authors also discuss the current challenges and future directions for limited data learning in NLP.|
|[Data augmentation for deep graph learning: A survey](https://dl.acm.org/doi/abs/10.1145/3575637.3575646)|ACM SIGKDD Explorations Newsletter 24.2 (2022)|The document is a survey on data augmentation techniques for deep graph learning. The authors highlight the challenges of data noise and data scarcity in deep graph learning and discuss the importance of data augmentation to address these issues. They propose a taxonomy for graph data augmentation techniques and categorize them based on the augmented information modalities. The authors also summarize the applications of graph data augmentation in reliable graph learning and low-resource graph learning. They provide a hierarchical problem taxonomy and review the existing literature related to graph data augmentation for each problem. The document concludes by pointing out promising research directions and challenges in future research.|
|[A survey of data augmentation approaches for NLP](https://arxiv.org/abs/2105.03075)|arXiv preprint arXiv:2105.03075 (2021)|The document is a survey of data augmentation approaches for natural language processing (NLP). It highlights the increased interest in data augmentation in NLP due to low-resource domains, new tasks, and the popularity of large-scale neural networks. The survey aims to provide a comprehensive overview of data augmentation techniques for NLP by summarizing the literature in a structured manner. It discusses rule-based, example interpolation-based, and model-based approaches. It also explores the use of data augmentation in various NLP applications and tasks. The document concludes by outlining current challenges and directions for future research in data augmentation for NLP.|
|[Demystifying Artificial Intelligence for Data Preparation](https://dl.acm.org/doi/10.1145/3555041.3589406)|SIGMOD-Companion â€™23|The document discusses the challenges and opportunities of using artificial intelligence (AI) for data preparation tasks. The authors highlight that data preparation is a time-consuming and error-prone process, and recent advances in AI have shown promising results in addressing these challenges. The document focuses on three important topics: 1. Foundation Models: The authors discuss the use of foundation models, such as GPT-3, which are trained on large corpora and have extensive knowledge. They explain how these models can be applied to various data preparation tasks, including data cleaning and entity resolution. They also discuss the limitations and opportunities of using foundation models. 2. Fine-tuning Pre-trained Language Models (PLMs): The document explains the concept of PLMs, such as BERT and RoBERTa, which are pre-trained on large corpora and can be fine-tuned for specific data preparation tasks. The authors discuss how PLMs can be used for tasks like blocking, entity matching, and schema matching. They also explain how PLMs can be adapted to new datasets and tasks.3. Data Preparation Pipeline Orchestration: The authors highlight the challenges of orchestrating data preparation pipelines, which involve a series of steps like data transformation and cleaning. They discuss different approaches, including manual pipeline orchestration, automatic pipeline generation, and human-in-the-loop pipeline generation. They provide insights into the existing approaches and open research problems in this area.|

**Others**
| Paper | Conference | Remark |
| :---:| :---:| :---:|
|[Chataug: Leveraging chatgpt for text data augmentation](https://arxiv.org/pdf/2302.13007v1.pdf)|arXiv preprint arXiv:2302.13007 (2023)|The document proposes a text data augmentation approach called ChatAug, which is based on the ChatGPT language model. The challenge of limited sample sizes in natural language processing (NLP) tasks, especially in few-shot learning scenarios, can be overcome by augmenting the training data. However, existing methods lack either correct labeling or diversity in the generated data. ChatAug rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples using ChatGPT. The augmented samples can then be used for downstream model training. Experimental results show that ChatAug outperforms state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of augmented samples. The document also discusses related work on data augmentation and few-shot learning. |
|[Data augmentation for spoken language understanding via pretrained language models](https://arxiv.org/abs/2004.13952)|arXiv preprint arXiv:2004.13952 (2020)|The document discusses a data augmentation method using pretrained language models to improve the training of spoken language understanding (SLU) models. The authors propose a semantically controlled generation approach, where a pretrained language model is used to generate synthetic training data based on dialogue acts. The method aims to increase the variability and accuracy of SLU models. Additionally, the authors address two overlooked scenarios of data scarcity in SLU: "Rich-in-Ontology" and "Rich-in-Utterance." In the Rich-in-Ontology scenario, the full ontology for the dialogue domain is given, but natural language utterances are lacking. In the Rich-in-Utterance scenario, there are abundant unlabelled utterances without annotated intents, slots, and values. The authors propose corresponding data augmentation solutions for both scenarios. Experimental results show that the proposed method outperforms baseline systems in terms of slot tagging and intent classification accuracies on the ATIS and Snips datasets.|
|[Effective Entity Augmentation By Querying External Data Sources](https://dl.acm.org/doi/abs/10.14778/3611479.3611535)|PVLDB 23|The document discusses the challenge of augmenting and enriching entities in datasets with relevant information from external data sources. It highlights the difficulties in formulating keyword queries to extract specific information from external sources and the limitations of current methods for information enrichment. The document proposes a progressive approach that leverages user feedback to learn how to retrieve information relevant to each entity in a dataset from external data sources. The approach aims to deliver relevant information quickly and with minimal expert intervention. The document also discusses the challenges of online learning of query policies and presents various methods and techniques to address these challenges. The proposed methods are evaluated through empirical studies using real-world datasets from different domains.|

